{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/sk-classroom/asc-recurrent-neural-nets/blob/main/assignments/assignment_02.ipynb)\n",
    "\n",
    "We will be creating a character-level LSTM trained on The Foundation by Isaac Asimov.  \n",
    "\n",
    "This character-level LSTM will read the book at character levels and be trained to predict the next character. By repeating the next character predictions, the LSTM will generate text in style similar to Isaac Asimov (if you train well).  \n",
    "\n",
    "# Data \n",
    "\n",
    "We will use the text in [archive.org](https://archive.org/stream/AsimovTheFoundation/Asimov_the_foundation_djvu.txt). The text is preprocessed and saved in \"data/the-foundation.txt\".  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# %% Extract the main content of the book\n",
    "## Reading and processing text\n",
    "with open(\"../data/the-foundation.txt\", \"r\", encoding=\"utf8\") as fp:\n",
    "    full_text_data = fp.read()\n",
    "\n",
    "# Remove linebreaks from the text\n",
    "full_text_data = full_text_data.replace(\"\\n\", \" \")\n",
    "full_text_data = full_text_data.replace(\"\\r\", \" \")\n",
    "full_text_data = full_text_data.replace(\"\\\\'\", \"'\")\n",
    "\n",
    "# Replace multiple spaces with a single space\n",
    "full_text_data = re.sub(r\"\\s+\", \" \", full_text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract the content of the book, and convert the text into a sequence of integers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_indx = full_text_data.find(\"THE PSYCHOHISTORIANS\")\n",
    "end_indx = len(full_text_data)\n",
    "\n",
    "full_text_data = full_text_data[start_indx:end_indx]\n",
    "char_set = set(full_text_data)\n",
    "print(\"Total Length:\", len(full_text_data))\n",
    "print(\"Unique Characters:\", len(char_set))\n",
    "\n",
    "# Tokenize the text into integers reresenting characters\n",
    "## Creating a lookup table\n",
    "chars = sorted(char_set)\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = np.array(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the book is too long, we will only use the first 10000 characters for the training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = full_text_data[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, let's convert the text into an integer sequence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array(list(map(lambda x: char_to_int[x], text_data)), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text_as_int contains the encoded values for all the characters in the text. Let's take a look at how part of our text is encoded:\n",
    "print(\"Text:\", text_data[:20], \"\\nEncoded:\", text_as_int[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset for training the Long-term short memory \n",
    "\n",
    "In this exercise, we will feed a fixed number of characters into the LSTM to predict the next character. While it is possible to feed all previous characters, it is not practical due to the memory and computation time. \n",
    "\n",
    "More concretely, for example, given text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"Fox jumps over the lazy dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by using five previous characters, we predict the next character, namely, \n",
    "\n",
    "| Input | Target | \n",
    "|-------|--------|\n",
    "| 'F', 'o', 'x', ' ', 'j' | 'u' |\n",
    "| 'o', 'x', ' ', 'j', 'u' | 'm' |\n",
    "| 'x', ' ', 'j', 'u', 'm' | 'p' |\n",
    "| ' ', 'j', 'u', 'm', 'p' | 's' |\n",
    "| 'j', 'u', 'm', 'p', 's' | ' ' |\n",
    "| 'u', 'm', 'p', 's', ' ' | 'o' |\n",
    "| 'm', 'p', 's', ' ', 'o' | 'v' |\n",
    "\n",
    "The input and target data can be created by striding the text by a fixed number of characters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def seq2input_target(seq, window_length):\n",
    "    input_text = [\n",
    "        list(seq[i : i + window_length]) for i in range(len(seq) - window_length)\n",
    "    ]\n",
    "    target_text = list(seq[window_length:])\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "chart_seq = list(example_text)\n",
    "\n",
    "print(\"Original alphabet sequence:\", chart_seq)\n",
    "\n",
    "inputs, targets = seq2input_target(chart_seq, 5)\n",
    "\n",
    "print(\"Input:\", inputs[:5])\n",
    "print(\"Target:\", targets[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is more efficient to use the integer representation of the text to create the input and target data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text_as_int = [char_to_int[ch] for ch in example_text]\n",
    "inputs, targets = seq2input_target(example_text_as_int, 5)\n",
    "\n",
    "print(\"Input:\", inputs[:5])\n",
    "print(\"Target:\", targets[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the seq2input_target function, define a torch dataset and data loader. In this exercise, we will use the length of 30 characters for the input. Set the batch size to 32~128. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# TODO: Create a dataset for the LSTM model\n",
    "class LSTMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, seq_data, window_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        seq_data: The sequence data.\n",
    "            length: The length of the sequence to be used for the input and target\n",
    "        window_length: int\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "\n",
    "dataset = LSTMDataset(text_as_int, window_length=15)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=512, shuffle=False, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's define the LSTM model. \n",
    "* Define the LSTM by using torch.nn. Module\n",
    "* In addition to the components of the LSTM, add a linear layer that converts the hidden state to the output of size equal to the number of unique characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# TODO: Define the LSTM model\n",
    "class LSTM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        # TODO: Define the gates\n",
    "\n",
    "        # TODO: Define the linear transformations between the cell states, inputs, and hidden states\n",
    "\n",
    "        # TODO: Define the linear layer that maps the hidden state to the output of size output_size\n",
    "\n",
    "        # TODO: Define the activation functions, Tanh and Sigmoid\n",
    "\n",
    "    def foward(self, input, hidden, cell):\n",
    "        # TODO: Define the forward pass\n",
    "\n",
    "\n",
    "\n",
    "lstm = LSTM(input_size=len(char_set), hidden_size=128, output_size=len(char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LSTM!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Define the loss and the optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to run the LSTM model on the sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define this function\n",
    "def run_ltsm(seqs, lstm, hidden_size):\n",
    "    \"\"\"Run the LSTM model on the sequences\n",
    "\n",
    "    Args:\n",
    "    seqs: The input sequences\n",
    "    lstm: The LSTM model\n",
    "    hidden_size: The size of the hidden states\n",
    "\n",
    "    Returns:\n",
    "    output: The output of the LSTM for the last character in the sequence\n",
    "    \"\"\"\n",
    "    n_seqs = seqs.shape[0]\n",
    "    seq_length = seqs.shape[1]\n",
    "\n",
    "    # TODO: Initialize the hidden and cell states\n",
    "    # Hint:\n",
    "    # - Use torch.zeros to initialize the hidden and cell states\n",
    "\n",
    "    # TODO: Run the LSTM model on the sequences\n",
    "    # Hint:\n",
    "    # - Use torch.nn.functional.one_hot to convert the input sequences to one-hot vectors\n",
    "    # - Then, feed the one-hot vectors to the LSTM model\n",
    "    return output\n",
    "\n",
    "\n",
    "example_text_as_int = [char_to_int[ch] for ch in example_text]\n",
    "inputs, targets = seq2input_target(example_text_as_int, 5)\n",
    "\n",
    "run_ltsm(torch.tensor(inputs[:3]), lstm, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the LSTM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_epochs = 1\n",
    "loss_values = []\n",
    "\n",
    "# TODO: Train the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the text by using the trained LSTM\n",
    "\n",
    "We will generate the text by using the trained LSTM. \n",
    "\n",
    "1. We will feed the first 30 characters into the LSTM.\n",
    "2. Use the output to predict the next character. \n",
    "3. Add the predicted character to the input and remove the first character. \n",
    "4. Repeat this process to generate the text. \n",
    "\n",
    "A word is selected randomly based on the probability of the next character calculated by the softmax function as follows:\n",
    "$$\n",
    "P(w | w_{t-1}, w_{t-2}, \\ldots, w_1) = \\frac{\\exp(\\text{LSTM}(w)/T)}{\\sum_{w'} \\exp(\\text{LSTM}(w')/T)}, \n",
    "$$\n",
    "where $T>0$ represents the temperature parameter that controls the randomness of the output. A large temperature value results in a more random output, while a small temperature value results in a more deterministic output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    lstm, start_text, length, window_length, char_to_int, int_to_char, temp\n",
    "):\n",
    "    \"\"\"Generate text using the LSTM model\n",
    "\n",
    "    Args:\n",
    "    lstm: The LSTM model\n",
    "    start_text: The starting text\n",
    "    length: The length of the generated text\n",
    "    window_length: The length of the input sequence\n",
    "    char_to_int: The character to integer mapping\n",
    "    int_to_char: The integer to character mapping\n",
    "\n",
    "    Returns:\n",
    "    generated_text: The generated text\n",
    "    \"\"\"\n",
    "    start_text_as_int = [char_to_int[ch] for ch in start_text]\n",
    "    generated_text = start_text_as_int\n",
    "\n",
    "    for i in range(length):\n",
    "        input_text = generated_text[-np.minimum(window_length, len(generated_text)) :]\n",
    "        input_text = torch.tensor(input_text).view(1, -1)\n",
    "        output = run_ltsm(input_text, lstm, 128)\n",
    "        output = torch.softmax(output / temp, dim=1).multinomial(num_samples=1)\n",
    "        generated_text.append(output.item())\n",
    "\n",
    "    generated_text = [int_to_char[i] for i in generated_text]\n",
    "    return \"\".join(generated_text)\n",
    "\n",
    "\n",
    "generate_text(lstm, \"My nam\", 100, 30, char_to_int, int_to_char, temp=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text would not be readable. Still, you might see some unique words used in the book. Such as \"Seldon\", \"Second Foundation\".\n",
    "\n",
    "Interested students are encouraged to expand the training data by changing the following line in the code at the third cell. \n",
    "\n",
    "```python \n",
    "#text_data = full_text_data[:10000] # old\n",
    "text_data = full_text_data[:1000000] # new \n",
    "#text_data = full_text_data[:10000000] # (if you have time) \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the learning is successful, the trained LTCM should be able to predict the next character better than a random prediction. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/the-foundation-test.txt\", \"r\", encoding=\"utf8\") as fp:\n",
    "    eval_text = fp.read()\n",
    "\n",
    "eval_text = eval_text.replace(\"\\n\", \" \")\n",
    "eval_text = eval_text.replace(\"\\r\", \" \")\n",
    "eval_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1747\n",
      "Random Accuracy: 0.0120\n"
     ]
    }
   ],
   "source": [
    "eval_text_as_int = [char_to_int[ch] for ch in eval_text]\n",
    "inputs, targets = seq2input_target(eval_text_as_int, window_length=30)\n",
    "\n",
    "output = run_ltsm(torch.tensor(inputs), lstm, 128)\n",
    "predictions = torch.argmax(output, dim=1).view(-1).to(\"cpu\").numpy()\n",
    "\n",
    "# To char\n",
    "predictions = np.array([int_to_char[i] for i in predictions])\n",
    "targets = np.array([int_to_char[i] for i in targets])\n",
    "\n",
    "acc = np.mean(predictions == np.array(targets))\n",
    "rand_acc = 1.0 / len(char_set)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Random Accuracy: {rand_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit the results\n",
    "\n",
    "Please **git commit & push** the following two files created in the following cell. \n",
    "\n",
    "1. \"~/assignments/lstm_test_predictions.txt\" \n",
    "\n",
    "2. \"~/assignments/lstm_loss_values.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./lstm_test_predictions.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    for char in predictions:\n",
    "        f.write(char)\n",
    "\n",
    "pd.DataFrame(loss_values).to_csv(\"./lstm_loss_values.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applsoftcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
